import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { processMarkdownAndChunk, Chunk } from '../utils/markdownProcessor.js';
import * as fs from 'fs/promises';
import * as path from 'path';
import { fileURLToPath } from 'url';
import 'dotenv/config';

// Define the structure for inserting into the v2 database table
interface DbEmbeddingRowV2 {
    content: string;
    metadata: Chunk['metadata']; // JSONB chunk-level metadata
    source_file: string;
    h1_heading: string | null;
    file_hash: string;
    last_modified: Date;
    // embedding will be generated by the Edge Function
}

// --- Configuration ---
// Get the directory name using the ES Module pattern
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Path to the directory containing individual Arcs markdown files
const ARCS_DATA_DIR = path.resolve(__dirname, '../../src/data/games/arcs');

// All 9 Arcs markdown files to process
const ARCS_MARKDOWN_FILES = [
    'arcs_rules_base_game.md',
    'arcs_rules_blighted_reach.md',
    'arcs_cards_base_game.md',
    'arcs_cards_blighted_reach.md',
    'arcs_cards_leaders_and_lore.md',
    'arcs_cards_errata.md',
    'arcs_faq_base_game.md',
    'arcs_faq_blighted_reach.md',
    'arcs_cards_faq.md'
];

const EMBEDDINGS_TABLE_NAME = 'arcs_rules_embeddings_v2'; // Updated to v2 table
const CLEAR_TABLE_BEFORE_INGEST = true; // Set to true to wipe table before adding new data
const PROCESSING_BATCH_SIZE = 50; // Number of chunks to process in each batch for the Edge Function

// --- End Configuration ---

// Initialize Supabase client
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !supabaseServiceKey) {
    throw new Error("Supabase URL and Service Role Key must be provided in environment variables.");
}

const supabaseAdmin: SupabaseClient = createClient(supabaseUrl, supabaseServiceKey, {
    auth: {
        autoRefreshToken: true,
        persistSession: false,
    }
});

/**
 * Generate embeddings using Supabase Edge Function
 */
async function generateEmbeddings(texts: string[]): Promise<number[][]> {
    const { data, error } = await supabaseAdmin.functions.invoke('generate-embeddings', {
        body: { texts }
    });

    if (error) {
        throw new Error(`Failed to generate embeddings: ${error.message}`);
    }

    if (!data || !data.embeddings) {
        throw new Error('Invalid response from generate-embeddings function');
    }

    return data.embeddings;
}

/**
 * Process a single markdown file and return chunks
 */
async function processMarkdownFile(filePath: string): Promise<Chunk[]> {
    try {
        const markdownContent = await fs.readFile(filePath, 'utf-8');
        console.log(`üìÑ Processing ${path.basename(filePath)}...`);
        
        // Use the enhanced processor with automatic metadata extraction
        const chunks = processMarkdownAndChunk(markdownContent, { filePath });
        console.log(`   Generated ${chunks.length} chunks`);
        
        return chunks;
    } catch (error: any) {
        console.error(`‚ùå Error processing ${filePath}: ${error.message}`);
        return [];
    }
}

/**
 * Convert chunks to database rows for v2 schema
 */
function prepareDbRows(chunks: Chunk[]): DbEmbeddingRowV2[] {
    return chunks.map(chunk => ({
        content: chunk.content,
        metadata: chunk.metadata, // JSONB chunk-level metadata
        source_file: chunk.fileMetadata.source_file,
        h1_heading: chunk.fileMetadata.h1_heading,
        file_hash: chunk.fileMetadata.file_hash,
        last_modified: chunk.fileMetadata.last_modified,
    }));
}

/**
 * Main ingestion function
 */
async function ingestData() {
    try {
        console.log('üöÄ Starting Arcs Rules Ingestion (v2 Schema)...\n');

        // 1. Verify all markdown files exist
        const validFiles: string[] = [];
        for (const fileName of ARCS_MARKDOWN_FILES) {
            const filePath = path.join(ARCS_DATA_DIR, fileName);
            try {
                await fs.access(filePath);
                validFiles.push(filePath);
            } catch (error) {
                console.warn(`‚ö†Ô∏è  File not found: ${fileName}`);
            }
        }

        if (validFiles.length === 0) {
            throw new Error(`No valid markdown files found in ${ARCS_DATA_DIR}`);
        }

        console.log(`üìö Found ${validFiles.length} markdown files to process\n`);

        // 2. Optional: Clear existing data
        if (CLEAR_TABLE_BEFORE_INGEST) {
            console.log('üßπ Clearing existing data from v2 table...');
            const { error: deleteError } = await supabaseAdmin
                .from(EMBEDDINGS_TABLE_NAME)
                .delete()
                .neq('id', '00000000-0000-0000-0000-000000000000'); // Delete all rows

            if (deleteError) {
                throw new Error(`Failed to clear table: ${deleteError.message}`);
            }
            console.log('‚úÖ Table cleared\n');
        }

        // 3. Process all markdown files
        const allChunks: Chunk[] = [];
        for (const filePath of validFiles) {
            const chunks = await processMarkdownFile(filePath);
            allChunks.push(...chunks);
        }

        if (allChunks.length === 0) {
            console.log('‚ö†Ô∏è  No chunks generated from any files');
            return;
        }

        console.log(`\nüìä Total processing summary:`);
        console.log(`   Total chunks: ${allChunks.length}`);
        
        // Show content type breakdown
        const contentTypeBreakdown = allChunks.reduce((acc, chunk) => {
            acc[chunk.metadata.content_type] = (acc[chunk.metadata.content_type] || 0) + 1;
            return acc;
        }, {} as Record<string, number>);
        
        Object.entries(contentTypeBreakdown).forEach(([type, count]) => {
            console.log(`   ${type}: ${count} chunks`);
        });

        // 4. Prepare database rows
        const dbRows = prepareDbRows(allChunks);

        // 5. Generate embeddings and insert data in batches
        console.log(`\nüî¢ Generating embeddings and inserting data...`);
        let successCount = 0;
        let errorCount = 0;

        for (let i = 0; i < dbRows.length; i += PROCESSING_BATCH_SIZE) {
            const batch = dbRows.slice(i, i + PROCESSING_BATCH_SIZE);
            const batchTexts = batch.map(row => row.content);
            
            try {
                console.log(`   Processing batch ${Math.floor(i / PROCESSING_BATCH_SIZE) + 1}/${Math.ceil(dbRows.length / PROCESSING_BATCH_SIZE)} (${batch.length} chunks)...`);
                
                // Generate embeddings using Supabase Edge Function
                const embeddings = await generateEmbeddings(batchTexts);
                
                // Prepare rows with embeddings for insertion
                const rowsWithEmbeddings = batch.map((row, index) => ({
                    ...row,
                    embedding: embeddings[index],
                }));
                
                // Insert into v2 table
                const { error: insertError } = await supabaseAdmin
                    .from(EMBEDDINGS_TABLE_NAME)
                    .insert(rowsWithEmbeddings);

                if (insertError) {
                    console.error(`‚ùå Failed to insert batch: ${insertError.message}`);
                    errorCount += batch.length;
                } else {
                    successCount += batch.length;
                    console.log(`   ‚úÖ Inserted ${batch.length} chunks`);
                }
            } catch (error: any) {
                console.error(`‚ùå Error processing batch: ${error.message}`);
                errorCount += batch.length;
            }
            
            // Add delay between batches to avoid overwhelming the Edge Function
            if (i + PROCESSING_BATCH_SIZE < dbRows.length) {
                await new Promise(resolve => setTimeout(resolve, 2000));
            }
        }

        // 6. Final summary
        console.log(`\nüéâ Ingestion Complete!`);
        console.log(`   ‚úÖ Successfully processed: ${successCount} chunks`);
        console.log(`   ‚ùå Failed to process: ${errorCount} chunks`);
        console.log(`   üìä Success rate: ${((successCount / (successCount + errorCount)) * 100).toFixed(1)}%`);
        
        if (successCount > 0) {
            console.log(`\nüîç Verifying data in ${EMBEDDINGS_TABLE_NAME}...`);
            const { count, error: countError } = await supabaseAdmin
                .from(EMBEDDINGS_TABLE_NAME)
                .select('*', { count: 'exact', head: true });
                
            if (!countError) {
                console.log(`   Database contains ${count} total rows`);
            }
        }

    } catch (error: any) {
        console.error(`üí• Ingestion failed: ${error.message}`);
        process.exit(1);
    }
}

// Run the ingestion function
ingestData();
