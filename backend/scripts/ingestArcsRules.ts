import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { processMarkdownAndChunk, Chunk } from '../utils/markdownProcessor.js';
import * as fs from 'fs/promises';
import * as path from 'path';
import { fileURLToPath } from 'url';
import crypto from 'crypto';
import 'dotenv/config';

// Define the structure for inserting into the v2 database table
interface DbEmbeddingRowV2 {
    content: string;
    metadata: Chunk['metadata']; // JSONB chunk-level metadata
    source_file: string;
    h1_heading: string | null;
    file_hash: string;
    last_modified: Date;
    // embedding will be generated by the Edge Function
}

// --- Configuration ---
// Get the directory name using the ES Module pattern
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Path to the directory containing individual Arcs markdown files
const ARCS_DATA_DIR = path.resolve(__dirname, '../../src/data/games/arcs');

// All 9 Arcs markdown files to process
const ARCS_MARKDOWN_FILES = [
    'arcs_rules_base_game.md',
    'arcs_rules_blighted_reach.md',
    'arcs_cards_base_game.md',
    'arcs_cards_blighted_reach.md',
    'arcs_cards_leaders_and_lore.md',
    'arcs_cards_errata.md',
    'arcs_faq_base_game.md',
    'arcs_faq_blighted_reach.md',
    'arcs_cards_faq.md'
];

const EMBEDDINGS_TABLE_NAME = 'arcs_rules_embeddings'; // Production table after v2 cutover
const PROCESSING_BATCH_SIZE = 5; // Number of chunks to process in each batch for the Edge Function (reduced for Free plan limits)

// --- End Configuration ---

// Initialize Supabase client
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !supabaseServiceKey) {
    throw new Error("Supabase URL and Service Role Key must be provided in environment variables.");
}

const supabaseAdmin: SupabaseClient = createClient(supabaseUrl, supabaseServiceKey, {
    auth: {
        autoRefreshToken: true,
        persistSession: false,
    }
});

/**
 * Generate embeddings using Supabase Edge Function
 */
async function generateEmbeddings(texts: string[]): Promise<number[][]> {
    const { data, error } = await supabaseAdmin.functions.invoke('generate-embeddings-native', {
        body: { inputText: texts }
    });

    if (error) {
        throw new Error(`Failed to generate embeddings: ${error.message}`);
    }

    if (!data || !data.embeddings) {
        throw new Error('Invalid response from generate-embeddings-native function');
    }

    return data.embeddings;
}

/**
 * Process a single markdown file and return chunks
 */
async function processMarkdownFile(filePath: string): Promise<Chunk[]> {
    try {
        const markdownContent = await fs.readFile(filePath, 'utf-8');
        console.log(`üìÑ Processing ${path.basename(filePath)}...`);
        
        // Use the enhanced processor with automatic metadata extraction
        const chunks = processMarkdownAndChunk(markdownContent, { filePath });
        console.log(`   Generated ${chunks.length} chunks`);
        
        return chunks;
    } catch (error: any) {
        console.error(`‚ùå Error processing ${filePath}: ${error.message}`);
        return [];
    }
}

/**
 * Convert chunks to database rows for v2 schema
 */
function prepareDbRows(chunks: Chunk[]): DbEmbeddingRowV2[] {
    return chunks.map(chunk => ({
        content: chunk.content,
        metadata: chunk.metadata, // JSONB chunk-level metadata
        source_file: chunk.fileMetadata.source_file,
        h1_heading: chunk.fileMetadata.h1_heading,
        file_hash: chunk.fileMetadata.file_hash,
        last_modified: chunk.fileMetadata.last_modified,
    }));
}

/**
 * File change detection result
 */
interface FileChangeStatus {
    filePath: string;
    fileName: string;
    status: 'new' | 'changed' | 'unchanged';
    currentHash: string;
    currentLastModified: Date;
    existingHash?: string;
    existingLastModified?: Date;
}

/**
 * Get existing file metadata from the database
 */
async function getExistingFileMetadata(): Promise<Map<string, { file_hash: string; last_modified: Date }>> {
    const { data, error } = await supabaseAdmin
        .from(EMBEDDINGS_TABLE_NAME)
        .select('source_file, file_hash, last_modified')
        .order('source_file');

    if (error) {
        console.warn(`‚ö†Ô∏è  Could not fetch existing file metadata: ${error.message}`);
        return new Map();
    }

    // Create a map of source_file -> latest metadata
    const fileMetadataMap = new Map<string, { file_hash: string; last_modified: Date }>();
    
    if (data) {
        data.forEach(row => {
            const existingEntry = fileMetadataMap.get(row.source_file);
            const currentLastModified = new Date(row.last_modified);
            
            // Keep the most recent entry for each file
            if (!existingEntry || currentLastModified > existingEntry.last_modified) {
                fileMetadataMap.set(row.source_file, {
                    file_hash: row.file_hash,
                    last_modified: currentLastModified
                });
            }
        });
    }

    return fileMetadataMap;
}

/**
 * Detect which files have changed and need re-ingestion
 */
async function detectFileChanges(filePaths: string[]): Promise<FileChangeStatus[]> {
    console.log('üîç Detecting file changes...');
    
    const existingMetadata = await getExistingFileMetadata();
    const changeStatuses: FileChangeStatus[] = [];
    
    for (const filePath of filePaths) {
        const fileName = path.basename(filePath);
        
        try {
            // Get current file metadata
            const fileContent = await fs.readFile(filePath, 'utf-8');
            const fileStats = await fs.stat(filePath);
            const currentHash = crypto.createHash('sha256').update(fileContent, 'utf8').digest('hex');
            const currentLastModified = fileStats.mtime;
            
            const existingFile = existingMetadata.get(fileName);
            
            let status: FileChangeStatus['status'];
            if (!existingFile) {
                status = 'new';
                console.log(`   üìÑ ${fileName}: NEW FILE`);
            } else if (existingFile.file_hash !== currentHash) {
                status = 'changed';
                console.log(`   üìù ${fileName}: CONTENT CHANGED (hash mismatch)`);
            } else if (currentLastModified > existingFile.last_modified) {
                status = 'changed';
                console.log(`   ‚è∞ ${fileName}: TIMESTAMP NEWER (${currentLastModified.toISOString()} > ${existingFile.last_modified.toISOString()})`);
            } else {
                status = 'unchanged';
                console.log(`   ‚úÖ ${fileName}: UNCHANGED`);
            }
            
            changeStatuses.push({
                filePath,
                fileName,
                status,
                currentHash,
                currentLastModified,
                existingHash: existingFile?.file_hash,
                existingLastModified: existingFile?.last_modified
            });
            
        } catch (error: any) {
            console.error(`‚ùå Error checking ${fileName}: ${error.message}`);
            // Treat as new file if we can't read metadata
            changeStatuses.push({
                filePath,
                fileName,
                status: 'new',
                currentHash: '',
                currentLastModified: new Date()
            });
        }
    }
    
    const newFiles = changeStatuses.filter(f => f.status === 'new').length;
    const changedFiles = changeStatuses.filter(f => f.status === 'changed').length;
    const unchangedFiles = changeStatuses.filter(f => f.status === 'unchanged').length;
    
    console.log(`\nüìä Change Detection Summary:`);
    console.log(`   üÜï New files: ${newFiles}`);
    console.log(`   üìù Changed files: ${changedFiles}`);
    console.log(`   ‚úÖ Unchanged files: ${unchangedFiles}`);
    console.log(`   üìã Total files checked: ${changeStatuses.length}\n`);
    
    return changeStatuses;
}

/**
 * Remove existing chunks for specific files from the database
 */
async function removeExistingChunks(sourceFiles: string[]): Promise<void> {
    if (sourceFiles.length === 0) return;
    
    console.log(`üßπ Removing existing chunks for ${sourceFiles.length} files...`);
    
    for (const sourceFile of sourceFiles) {
        const { error } = await supabaseAdmin
            .from(EMBEDDINGS_TABLE_NAME)
            .delete()
            .eq('source_file', sourceFile);
            
        if (error) {
            console.error(`‚ùå Error removing chunks for ${sourceFile}: ${error.message}`);
        } else {
            console.log(`   ‚úÖ Removed existing chunks for ${sourceFile}`);
        }
    }
}

/**
 * Main ingestion function with incremental change detection
 */
async function ingestData() {
    try {
        console.log('üöÄ Starting Arcs Rules Incremental Ingestion (v2 Schema)...\n');

        // 1. Verify all markdown files exist
        const validFiles: string[] = [];
        for (const fileName of ARCS_MARKDOWN_FILES) {
            const filePath = path.join(ARCS_DATA_DIR, fileName);
            try {
                await fs.access(filePath);
                validFiles.push(filePath);
            } catch (error) {
                console.warn(`‚ö†Ô∏è  File not found: ${fileName}`);
            }
        }

        if (validFiles.length === 0) {
            throw new Error(`No valid markdown files found in ${ARCS_DATA_DIR}`);
        }

        console.log(`üìö Found ${validFiles.length} markdown files to check\n`);

        // 2. Detect file changes
        const fileChanges = await detectFileChanges(validFiles);
        const filesToProcess = fileChanges.filter(f => f.status === 'new' || f.status === 'changed');
        
        if (filesToProcess.length === 0) {
            console.log('üéâ All files are up to date! No ingestion needed.');
            return;
        }

        console.log(`üìù Processing ${filesToProcess.length} files that need updates:\n`);
        filesToProcess.forEach(f => {
            console.log(`   ${f.status === 'new' ? 'üÜï' : 'üìù'} ${f.fileName} (${f.status.toUpperCase()})`);
        });
        console.log();

        // 3. Remove existing chunks for files that are being re-ingested
        const filesToReprocess = fileChanges.filter(f => f.status === 'changed').map(f => f.fileName);
        if (filesToReprocess.length > 0) {
            await removeExistingChunks(filesToReprocess);
        }

        // 4. Process only changed/new markdown files
        const allChunks: Chunk[] = [];
        for (const fileChange of filesToProcess) {
            const chunks = await processMarkdownFile(fileChange.filePath);
            allChunks.push(...chunks);
        }

        if (allChunks.length === 0) {
            console.log('‚ö†Ô∏è  No chunks generated from any files');
            return;
        }

        console.log(`\nüìä Total processing summary:`);
        console.log(`   Total chunks: ${allChunks.length}`);
        
        // Show content type breakdown
        const contentTypeBreakdown = allChunks.reduce((acc, chunk) => {
            acc[chunk.metadata.content_type] = (acc[chunk.metadata.content_type] || 0) + 1;
            return acc;
        }, {} as Record<string, number>);
        
        Object.entries(contentTypeBreakdown).forEach(([type, count]) => {
            console.log(`   ${type}: ${count} chunks`);
        });

        // 4. Prepare database rows
        const dbRows = prepareDbRows(allChunks);

        // 5. Generate embeddings and insert data in batches
        console.log(`\nüî¢ Generating embeddings and inserting data...`);
        let successCount = 0;
        let errorCount = 0;

        for (let i = 0; i < dbRows.length; i += PROCESSING_BATCH_SIZE) {
            const batch = dbRows.slice(i, i + PROCESSING_BATCH_SIZE);
            const batchTexts = batch.map(row => row.content);
            
            try {
                console.log(`   Processing batch ${Math.floor(i / PROCESSING_BATCH_SIZE) + 1}/${Math.ceil(dbRows.length / PROCESSING_BATCH_SIZE)} (${batch.length} chunks)...`);
                
                // Generate embeddings using Supabase Edge Function
                const embeddings = await generateEmbeddings(batchTexts);
                
                // Prepare rows with embeddings for insertion
                const rowsWithEmbeddings = batch.map((row, index) => ({
                    ...row,
                    embedding: embeddings[index],
                }));
                
                // Insert into v2 table
                const { error: insertError } = await supabaseAdmin
                    .from(EMBEDDINGS_TABLE_NAME)
                    .insert(rowsWithEmbeddings);

                if (insertError) {
                    console.error(`‚ùå Failed to insert batch: ${insertError.message}`);
                    errorCount += batch.length;
                } else {
                    successCount += batch.length;
                    console.log(`   ‚úÖ Inserted ${batch.length} chunks`);
                }
            } catch (error: any) {
                console.error(`‚ùå Error processing batch: ${error.message}`);
                errorCount += batch.length;
            }
            
            // Add delay between batches to avoid overwhelming the Edge Function
            if (i + PROCESSING_BATCH_SIZE < dbRows.length) {
                await new Promise(resolve => setTimeout(resolve, 2000));
            }
        }

        // 6. Final summary
        console.log(`\nüéâ Ingestion Complete!`);
        console.log(`   ‚úÖ Successfully processed: ${successCount} chunks`);
        console.log(`   ‚ùå Failed to process: ${errorCount} chunks`);
        console.log(`   üìä Success rate: ${((successCount / (successCount + errorCount)) * 100).toFixed(1)}%`);
        
        if (successCount > 0) {
            console.log(`\nüîç Verifying data in ${EMBEDDINGS_TABLE_NAME}...`);
            const { count, error: countError } = await supabaseAdmin
                .from(EMBEDDINGS_TABLE_NAME)
                .select('*', { count: 'exact', head: true });
                
            if (!countError) {
                console.log(`   Database contains ${count} total rows`);
            }
        }

    } catch (error: any) {
        console.error(`üí• Ingestion failed: ${error.message}`);
        process.exit(1);
    }
}

// Export functions for use by selective ingestion service
export { detectFileChanges, removeExistingChunks };

// Run the ingestion function if called directly (not imported)
const isMainModule = import.meta.url === `file://${process.argv[1]}`;
if (isMainModule) {
    ingestData();
}
